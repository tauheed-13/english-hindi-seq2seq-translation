{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Numerical operations and array handling\n",
        "import numpy as np\n",
        "\n",
        "# DataFrame handling (dataset is assumed to be a pandas DataFrame)\n",
        "import pandas as pd\n",
        "\n",
        "# Converts text into integer sequences (word → index)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Makes all sequences same length by padding with zeros\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Core neural network layers\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "\n",
        "# Used to build models using Functional API (needed for encoder–decoder)\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "19R36YmQBVi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Minimal cleaning for translation tasks.\n",
        "    We avoid removing punctuation aggressively because it\n",
        "    can change sentence meaning in translation.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Lowercase for consistency, strip removes extra spaces\n",
        "    return text.lower().strip()"
      ],
      "metadata": {
        "id": "skHBur5RCbql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/Hindi_English_Truncated_Corpus.csv')\n",
        "dataset = dataset.dropna().drop_duplicates()\n",
        "dataset = dataset[dataset['source'] == 'ted'][['english_sentence','hindi_sentence']]"
      ],
      "metadata": {
        "id": "QLdaxodiCjCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean English sentences (encoder input)\n",
        "dataset['english_sentence'] = dataset['english_sentence'].apply(clean_text)\n",
        "\n",
        "# Add start_ and _end tokens to Hindi sentences\n",
        "# Decoder learns where to start and stop generation\n",
        "dataset['hindi_sentence'] = dataset['hindi_sentence'].apply(\n",
        "    lambda x: 'start_ ' + clean_text(x) + ' _end'\n",
        ")"
      ],
      "metadata": {
        "id": "YMMyl00zCeUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# English tokenizer\n",
        "# - num_words limits vocabulary size\n",
        "# - <OOV> handles unseen words\n",
        "eng_token = Tokenizer(num_words=15000, oov_token=\"<OOV>\")\n",
        "eng_token.fit_on_texts(dataset['english_sentence'])\n",
        "\n",
        "# Hindi tokenizer\n",
        "# - filters='' prevents accidental removal of Devanagari characters\n",
        "hin_token = Tokenizer(num_words=15000, filters='', oov_token=\"<OOV>\")\n",
        "hin_token.fit_on_texts(dataset['hindi_sentence'])"
      ],
      "metadata": {
        "id": "1KUxAt0zCskk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert English sentences into sequences of token IDs\n",
        "eng_seq = eng_token.texts_to_sequences(dataset['english_sentence'])\n",
        "\n",
        "# Convert Hindi sentences into sequences of token IDs\n",
        "hin_seq = hin_token.texts_to_sequences(dataset['hindi_sentence'])"
      ],
      "metadata": {
        "id": "cC411vLjCvNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find maximum sequence lengths\n",
        "max_eng_len = max(len(seq) for seq in eng_seq)\n",
        "max_hin_len = max(len(seq) for seq in hin_seq)\n",
        "\n",
        "# Pad English sequences (encoder input)\n",
        "encoder_input = pad_sequences(\n",
        "    eng_seq,\n",
        "    maxlen=max_eng_len,\n",
        "    padding='post'  # add zeros at the end\n",
        ")\n",
        "\n",
        "# Pad Hindi sequences (decoder input)\n",
        "decoder_input = pad_sequences(\n",
        "    hin_seq,\n",
        "    maxlen=max_hin_len,\n",
        "    padding='post'\n",
        ")"
      ],
      "metadata": {
        "id": "p4JkBfg2Cxrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create empty target array\n",
        "# Shape: (samples, time_steps, 1)\n",
        "decoder_target = np.zeros(\n",
        "    (decoder_input.shape[0], decoder_input.shape[1], 1)\n",
        ")\n",
        "\n",
        "# Shift decoder input by one timestep (Teacher Forcing Learning)\n",
        "# Decoder learns: given word_t → predict word_(t+1)\n",
        "decoder_target[:, :-1, 0] = decoder_input[:, 1:]"
      ],
      "metadata": {
        "id": "zZpwXfYsC0uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary size = number of unique words + padding token\n",
        "eng_vocab_size = len(eng_token.word_index) + 1\n",
        "hin_vocab_size = len(hin_token.word_index) + 1\n",
        "\n",
        "# Latent dimension controls embedding size and LSTM memory capacity\n",
        "latent_dim = 256"
      ],
      "metadata": {
        "id": "HTXUmBn-C9JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder input receives a sequence of English token IDs\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# Embedding layer converts token IDs → dense vectors\n",
        "# mask_zero=True ensures padding tokens are ignored\n",
        "enc_emb = Embedding(\n",
        "    eng_vocab_size,\n",
        "    latent_dim,\n",
        "    mask_zero=True\n",
        ")(encoder_inputs)\n",
        "\n",
        "# LSTM processes the embedded sequence\n",
        "# return_state=True returns final hidden and cell states\n",
        "_, state_h, state_c = LSTM(\n",
        "    latent_dim,\n",
        "    return_state=True\n",
        ")(enc_emb)\n",
        "\n",
        "# Encoder states summarize the entire input sentence\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "QeluuPEODAAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder input receives Hindi token IDs\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "# Shared embedding layer for decoder\n",
        "dec_emb_layer = Embedding(\n",
        "    hin_vocab_size,\n",
        "    latent_dim,\n",
        "    mask_zero=True\n",
        ")\n",
        "\n",
        "# Convert decoder input tokens to vectors\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Decoder LSTM generates output at each timestep\n",
        "dec_lstm = LSTM(\n",
        "    latent_dim,\n",
        "    return_sequences=True,  # output at every timestep\n",
        "    return_state=True\n",
        ")\n",
        "\n",
        "# Initialize decoder LSTM with encoder states\n",
        "dec_outputs, _, _ = dec_lstm(\n",
        "    dec_emb,\n",
        "    initial_state=encoder_states\n",
        ")\n",
        "\n",
        "# Dense layer converts LSTM output → vocabulary probabilities\n",
        "dec_dense = Dense(hin_vocab_size, activation='softmax')\n",
        "dec_outputs = dec_dense(dec_outputs)"
      ],
      "metadata": {
        "id": "BlUCAUa_DClc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full training model (encoder + decoder)\n",
        "model = Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    dec_outputs\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "# - rmsprop works well for RNNs\n",
        "# - sparse_categorical_crossentropy because targets are integer IDs\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "cpmwoklHDE_Z",
        "outputId": "09956a1f-3d73-4aac-d258-4cefa356c2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m4,244,480\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m7,023,872\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m525,312\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m525,312\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m7,051,309\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│                     │ \u001b[38;5;34m27437\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">4,244,480</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,023,872</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,051,309</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">27437</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,370,285\u001b[0m (73.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,370,285</span> (73.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,370,285\u001b[0m (73.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,370,285</span> (73.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train using teacher forcing\n",
        "model.fit(\n",
        "    [encoder_input, decoder_input],\n",
        "    decoder_target,\n",
        "    batch_size=64,\n",
        "    epochs=1,\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdGHjIQjDHKo",
        "outputId": "9a6cd4bd-9726-4325-feb1-0d4c8d6de069"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m486/486\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3686s\u001b[0m 8s/step - accuracy: 0.6755 - loss: 6.9035 - val_accuracy: 0.7446 - val_loss: 5.7782\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f6fa86d7dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# During inference, encoder outputs only the final states\n",
        "encoder_model_inf = Model(\n",
        "    encoder_inputs,\n",
        "    encoder_states\n",
        ")"
      ],
      "metadata": {
        "id": "UrJ4xeSsDJfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs for previous decoder states\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "\n",
        "decoder_states_inputs = [\n",
        "    decoder_state_input_h,\n",
        "    decoder_state_input_c\n",
        "]\n",
        "\n",
        "# Embed the current decoder input token\n",
        "dec_inf_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Run one timestep of decoder LSTM\n",
        "dec_outputs_inf, state_h_inf, state_c_inf = dec_lstm(\n",
        "    dec_inf_emb,\n",
        "    initial_state=decoder_states_inputs\n",
        ")\n",
        "\n",
        "# Convert output to word probabilities\n",
        "decoder_outputs_inf = dec_dense(dec_outputs_inf)\n",
        "\n",
        "# Inference decoder model\n",
        "decoder_model_inf = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs_inf, state_h_inf, state_c_inf]\n",
        ")"
      ],
      "metadata": {
        "id": "0A52yzHCDNDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map token IDs back to words for readable output\n",
        "reverse_eng = {v: k for k, v in eng_token.word_index.items()}\n",
        "reverse_hin = {v: k for k, v in hin_token.word_index.items()}"
      ],
      "metadata": {
        "id": "lBz6MaFSDPlY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    # Clean input sentence\n",
        "    sentence = clean_text(sentence)\n",
        "\n",
        "    # Convert sentence to token sequence\n",
        "    seq = eng_token.texts_to_sequences([sentence])\n",
        "\n",
        "    # Pad to encoder input length\n",
        "    padded = pad_sequences(seq, maxlen=max_eng_len, padding='post')\n",
        "\n",
        "    # Encode sentence → initial decoder states\n",
        "    states = encoder_model_inf.predict(padded)\n",
        "\n",
        "    # Start decoding with start_ token\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = hin_token.word_index['start_']\n",
        "\n",
        "    decoded_words = []\n",
        "\n",
        "    while True:\n",
        "        # Predict next word\n",
        "        output, h, c = decoder_model_inf.predict(\n",
        "            [target_seq] + states\n",
        "        )\n",
        "\n",
        "        # Choose word with highest probability\n",
        "        token_index = np.argmax(output[0, -1, :])\n",
        "        word = reverse_hin.get(token_index, '')\n",
        "\n",
        "        # Stop if end token or max length reached\n",
        "        if word == '_end' or len(decoded_words) >= max_hin_len:\n",
        "            break\n",
        "\n",
        "        decoded_words.append(word)\n",
        "\n",
        "        # Feed predicted word back into decoder\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = token_index\n",
        "\n",
        "        # Update decoder states\n",
        "        states = [h, c]\n",
        "\n",
        "    return ' '.join(decoded_words)"
      ],
      "metadata": {
        "id": "ZOq5UCNSDSVx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"English:\", \"And\")\n",
        "print(\"Hindi:\",\n",
        "      translate(\"And\")\n",
        "     )"
      ],
      "metadata": {
        "id": "32PUMmjFDUdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd1fd62-490c-453b-9f41-4836ea0a9650"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: And\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "Hindi: और\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ghEOiqaLTehc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}